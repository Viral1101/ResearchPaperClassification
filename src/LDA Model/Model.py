from keras.models import Sequential, load_model, save_model
from keras.layers import Dense
from keras import optimizers
import nltk
import gensim
import numpy as np
import Processing


def get_model(filename):
    try:
        model = load_model(filename)
        return model
    except OSError:
        return create_model(filename)


def create_model(filename):
    model = Sequential()
    model.add(Dense(1024, input_dim=1024))
    model.add(Dense(1024, activation='sigmoid'))
    model.add(Dense(3, activation='softmax'))
    sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
    model.compile(loss='mean_squared_error', optimizer=sgd)

    if filename:
        save_model(model, filename)

    return model


def classify(x):
    model_filename = 'model.h5'
    sequential_model = get_model(model_filename)

    predictions = sequential_model.predict(x)

    classifications = list()
    for prediction in predictions:
        classifications.append(np.argmax(prediction))

    return classifications


def train_model(x, y):
    model_filename = 'model.h5'
    sequential_model = get_model(model_filename)

    sequential_model.fit(x, y)
    sequential_model.save(model_filename)


def create_input_vector(filename):
    filenames = [filename]

    preprocessed_files = list()
    for file in filenames:
        preprocessed_files.append(Processing.preprocess_file(file))

    # Find most used n-grams in the file
    top_ngram_list = find_top_ngrams(preprocessed_files[0], 100)

    # Separate topics by LDA
    dictionary = gensim.corpora.Dictionary(preprocessed_files)
    bow_corpus = [dictionary.doc2bow(file) for file in preprocessed_files]

    topic_amount = 3
    topic_list = list()

    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=topic_amount, id2word=dictionary, passes=100, workers=2)

    # Save Topics generated by LDA
    for i in range(0, topic_amount):
        word_list = list()
        words = lda_model.show_topic(i, 10)
        for word in words:
            word_list.append(word[0])
        topic_list.append(word_list)

    # Create word2vec model from PDFs
    word2vec_model = gensim.models.Word2Vec(preprocessed_files, size=256, window=5, min_count=1, workers=4)

    # Encode most found n-grams using word2vec model
    top_ngrams_encoded = list()
    for ngram in top_ngram_list:
        top_ngrams_encoded.append(Processing.encode_ngram(word2vec_model, ngram))

    # Encode Topics using word2vec model
    topic_list_encoded = list()
    for i in range(0, topic_amount):
        word_list = list()
        words = lda_model.show_topic(i, 10)
        for word in words:
            word_list.append(word[0])
        topic_list_encoded.append(Processing.encode_ngram(word2vec_model, word_list))


    # Tokenize the data by sentences
    sentence_list_encoded = list()

    sentences = Processing.get_sentences(Processing.readPDF(filename))
    for sentence in sentences:
        i = 0
        sentence_list_encoded.append(np.zeros(256))
        words = Processing.get_words(sentence)
        for ngram in top_ngram_list:
            if set(ngram) <= set(words):
                index = top_ngram_list.index(ngram)
                ngram_encoded = top_ngrams_encoded[index]
                for j in range(0, 256):
                    sentence_list_encoded[i][j] += ngram_encoded[j]
        i += 1

    encoded_input = list()
    for i in range(0, len(sentence_list_encoded)):
        row = list()
        row.extend(sentence_list_encoded[i])
        for topic in topic_list_encoded:
            row.extend(topic)
        encoded_input.append(row)

    return np.array(encoded_input)


def find_top_ngrams(preprocessed_file, ngram_count):
    # Find ngrams of sizes 3-10
    ngram_list = list()
    for i in range(3, 11):
        ngrams = nltk.ngrams(preprocessed_file, i)
        for ngram in ngrams:
            ngram_list.append(ngram)

    # Extract the top n-grams based on their count
    ngram_dict = dict()
    for ngram in ngram_list:
        if ngram not in ngram_dict.keys():
            ngram_dict[ngram] = 1
        else:
            ngram_dict[ngram] += 1

    top_ngram_list = list()
    for i in range(0, ngram_count):
        max_count = 0
        max_ngram = 0
        for ngram in ngram_dict.keys():
            if ngram_dict[ngram] > max_count:
                max_count = ngram_dict[ngram]
                max_ngram = ngram
                ngram_dict[ngram] = 0
        top_ngram_list.append(max_ngram)

    return top_ngram_list


def get_sentence_list(filename):
    data = Processing.readPDF(filename)
    sentences = Processing.get_sentences(data)
    return sentences


def get_classification_list(filename):
    x = create_input_vector(filename)
    return classify(x)
